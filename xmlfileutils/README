This is a small set of tools to help folks who want to import
XML dumps into a local instance of MediaWiki.

Tools:

mwxml2sql

This is a bare bones converter of MediaWiki XML dumps to sql.
It does one thing and hopefully at least doesn't suck at it.
If you want the swiss army knife of converters, better shop
elsewhere. This program is not intended to process upload xml
elements, nor logging xml elements.  It works with the
xml dumps from the Wikimedia projects and that is all
that it's intended to do.

To install and run this program, you will need to have the libz and
bz2 development libraries installed, as well as the gcc
toolchain or an equivalent C compiler and its supporting
libraries. You'll also need the 'make' utility.

sql2txt

This converts sql produced from mysqldump or from an XML dump to
sql file converter, into a format that can be read by MySQL via
LOAD DATA INFILE, a much faster process than having mysql do a pile
of INSERTS. If you feed it data formatted differently from that
output, or with rows broken across lines etc, it may produce garbage.

To install and run this program, you will need to have the libz and
bz2 development libraries installed, as well as the gcc
toolchain or an equivalent C compiler and its supporting
libraries. You'll also need the 'make' utility.

fifo_to_mysql.pl

This reads a tab-delimited file of table records, writes it
in chunks to a specified fifo, and forks mysql to LOAD DATA INFILE
from that fifo for each chunk.

To install and run this program, you will need to have the libz and
bz2 libraries installed, as well as the perl IO::Compress::Gzip,
IO::Compress::Bzip2, Fcntl and POSIX modules. These are core modules
so they should come by default with your perl package. You'll also
need the 'make' utility.

These programs have been tested only on 64-bit Linux and on FreeBSD.
You can try building them on other platforms but without any support
from the author.  If you do build them on another platform and
run it successfully, let us know and we'll include that
information here.

INSTALLATION

Unpack the distribution someplace, cd into it, 'make'
and if you like 'make install'.  The binaries will be
installed into /usr/local/bin.  If you want them someplace
else, edit the Makefile and change the line
'PREFIX=/usr/local'  as desired.

RUNNING

A. mwxml2sql

'mwxml2sql --help' will give you a comprehensive usage
message.

TL;DR version:  you need a page xml.bz2/gz file and a
stubs xml bz2/gz file for input, sql files for the page,
revision and text tables will be produced on output.

Example use:

mwxml2sql -v -s testing/enwiki-stubs-1114-1300.gz \
         -t testing/enwiki-pages-1114-1300.gz  \
         -f testing/sql_enwiki-1114-1300.gz -p t_ -m 1.20

In this example, we have:
-v: print progress messages
-s: stubs file for input
-t: text (page content) file for input
-f: filename used to build the page/revision/text output filenames
    in this case the files sql_enwiki-1114-1300-page.sql.gz,
    sql_enwiki-1114-1300-revs.sql.gz and sql_enwiki-1114-1300-text.sql.gz
    are produced
-p: prefix of tables in your database (correpsonds to $wgDBprefix
    in LocalSetting.php, leave out this option if you have no prefix)
-m: MediaWiki version for which to produce output

This program took about 1.5 hours to run on the enwikipedia
stubs and page files from December 2012, producing gzipped output
files for MediaWiki version 1.20.  Total pages (and therefore
revisions) processed: 12680605. Hardware: Sony laptop with
a linux desktop running (Xfce, gnome, evolution and pidgin running
but not actively doing other tasks).  Dual core P8700, 2.53GHz,
8GB memory, HTS723232L9SA60 disk with 72000 rpm.  Size of the
gzipped output files: 460M for page table, 905M for revision table,
9.6GB for text table.

I ran mwdumper on this same file on the same hardware and it took
just over two hours.  Somewhere there is a small memory leak,
not enough to concern someone working with the current page content
only.

A run of mwimport (perl script) on the same hardware took slightly
over two hours.

B. sql2txt

'sql2txt --help' will give you a comprehensive usage
message.

TL;DR version:  you need a possibly compressed (gz or bz2) file
containing sql insert statements; provide this to sql2txt and
save the tab delimited and escaped results to a (possibly
compressed) file.

Example use:

sql2txt  -v -s testing/enwiki-iwlinks.sql.bz2 \
         -t testing/enwiki-iwlinks.tabs.bz2

In this example, we have:
-v: print progress messages
-s: sql file for input
-t: tab delimited text file for output

C. fifo_to_mysql.pl

'fifo_to_mysql.pl --help' will give you a comprehensive usage
message.

TL;DR version:  you need a possibly compressed (gz or bz2) file
containing tab-delimted and escaped data for an sql table;
stuff your MySQL password in a conf file, provide fifo_to_mysql.pl
with the sql filename, the conf file name, the db and table names
and the number of bytes or lines to send to mysql per chunk, and
it will write chunks of the data to a fifo, spawning mysql to
LOAD DATA INFILE from that fifo for each chunk.

Example use:

for i in pagelinks page_props page redirect revision site_stats templatelinks text; do \
    perl fifo_to_mysql.pl --db elwikt --table ${i} --sqlfile ../dump-restore/dump_${i}.tabs.bz2 \
         --verbose --verbose --charset binary --mysqlopts unique_checks,foreign_key_checks \
        --fifo /var/lib/mysql/fifos/mysql_fifo --passwdfile mysql-client-opts.cf; done

In this example, we have:
--db:          name of database into which to load data
--table:       name of table into which to load data
--sqlfile:     name of tab delimited and escaped data for the table
--verbose:     print progress messages
--charset:     character set for the server to use when loading the file
               (binary indicates no conversion, this is likely safe)
--mysqlopts:   session settings to turn off, for speeed
--fifo:        path to fifo which mysql will read from
--passwdfile:  mysql config file which will have the password of the user

USING THE OUTPUT

Making a local mirror with all current pages

The sql files produced keep the page, revision and text ids of the
input XML files.  This means that if your database uses only XML
files from a single source and project (e.g. only the WMF en wikipedia
dumps), you will not have an id conflict but if you edit locally
between imports or you import on top of an existing database you
will encounter problems.

Convert it and all other needed sql tables to the format needed
for LOAD DATA INFILE, using the command

zcat blah.sql.gz | sql2txt > blah.gz
(see 'RUNNING' for more on this command).  You may decide you would
rather use bz2 which will be slower but will save a lot of disk space.

You can skip converting the image, imagelinks, oldimage and
user_groups tables.  Loading in the other tables saves you from
needing to rebuild things like the links tables afterwards
(horribly slow).

Make sure the database is set up with the right character set
(we like 'binary'), set your client character set (utf-8) and
change any settings you want to for speed (turning off for
example foreign_key_checks and/or unique_checks).

Now load your files in one at a time. (Tested on MySQL 5.5,
probably ok on 5.1, probably broken on 4.x.) If yu are working
with large (> 500MB) files you may want to feed them to mysql
in chunks.  You can do this by using the fifo_to_mysql.pl
perl script; see the section 'RUNNING' for more on this command.

If you don't have a ton of space you may want to convert your
sql files one at a time and remove the output file after
MySQL has read it in.

Note that the other sql tables are not guaranteed to be 100% consistent
with the XML files, since they are produced at different times and we
don't lock the tables before dumping.  Null edits to a page ought to
fix any rendering issues it may have based on out of sync tables.

Making a local mirror with a subset of current pages

If you want to import only a subset of the current pages from a wiki
instead of the entire thing, these are NOT the tools for you. You should
consider using maintenance/importDump.php (in your MediaWiki installation
directory) instead, if the subset is small enough.  Converting XML dumps
to page/rev/text tables and then loading in the other tables from our
generated sql files will give you a lot of links and entries that won't
be valid, and the other option, maintenance/rebuildall.php, will also be
very slow.

If you want to import files while keeping pre-existing content, these
are likewise NOT the tools for you; they will cause mysql to whine
about index conflicts due to page, revision or text ids already in
your database that are included in your sql files to be loaded. Consider
using maintenance/importDump.php (in your MediaWiki installation
directory) instead.

WARNINGS

This has been tested only for output MediaWiki 1.20 and input xsd 0.7
and not very comprehensively.  Please help discover bugs!

This does NOT support dumps from wikis with LiquidThread enabled.
That's a feature set for a future version (or not).

Other untested features: importing output with gzipped text revisions,
using history xml dumps.

Changes to the xml schema, in particular new tags or a switch in
the order of items, will break the script.

LICENSE

The files sha1.c and sha1.h are released by Christophe Levine under
GPLv2 (see the file COPYING in this directory).  His web site is
no longer available and the code has since been folded into many
other projects but you can find it via archive.org:
http://web.archive.org/web/20031123112259/http://www.cr0.net:8040/code/crypto/sha1/

The remaining files are copyright Ariel Glenn 2013 and also released
under the GPLv2 (see again the file COPYING in this directory).
